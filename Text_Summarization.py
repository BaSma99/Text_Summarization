# -*- coding: utf-8 -*-
"""NLP_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NO-O0afeSGBgMnsTtReFoBTpSsUquy8-

# **Group 15 Final Project Text Summarization**

link for the dataset: https://drive.google.com/drive/folders/1FxEZ8ffrGLrub318L2LO2y-x10k3VlhS?usp=sharing

## **Install dependancies and Import Important libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pytextrank
#Install mlxtend which is a library of Python tools and extensions for data science.
# %pip install mlxtend --upgrade
!pip uninstall matplotlib
!pip install matplotlib==3.1.3
!pip install yellowbrick
!pip install sumy
!pip install transformers
!python -m spacy download en_core_web_lg
!pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl
!pip3 install torchvision
!pip install transformers==4.12.4 sentencepiece
!python -m spacy download en_core_web_sm

!pip install torch==1.2.0 torchvision==0.4.0 -f
!pip install -U sentence-transformers
!pip install rouge-score

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# Loading Essential libraries 
import warnings
from transformers import *
from transformers import BertTokenizer, TFBertForQuestionAnswering
from transformers import BertForQuestionAnswering
from transformers import BertTokenizer
from transformers import AutoTokenizer
from sklearn.decomposition import TruncatedSVD #Singular value decomposition
from scipy.sparse import csr_matrix
from numpy import concatenate
from sumy.parsers.plaintext import PlaintextParser
from yellowbrick.cluster import KElbowVisualizer
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
warnings.filterwarnings('ignore')
# %config InlineBackend.figure_format = 'retina'
# %config Completer.use_jedi = False # this to force autocompletion 
from matplotlib import pyplot as plt
import seaborn as sns
import spacy
import en_core_web_sm
#import nltk---->leading platform for building Python programs to work with human language data.
import nltk
from collections import OrderedDict
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from scipy.sparse import csr_matrix
from numpy import concatenate
from yellowbrick.cluster import KElbowVisualizer
nltk.download('punkt')    #downloading punctuations from NLTK
nltk.download("stopwords")  #download stopwords from NLTK
nltk.download("wordnet")  #downloading lemmatizers from NLTK
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import gutenberg #a small selection of texts from the Project Gutenberg electronic text archive
from nltk.corpus import stopwords #importing stopwords
from nltk.stem import WordNetLemmatizer # algorithmic process of finding the lemma of a word depending on its meaning and context. 
import math #This module provides access to the mathematical functions defined by the C standard.
import pandas as pd  #open source data analysis library 
import numpy as np   #a Python library used for working with arrays.
import re #a Python library used for working with regular expressions
from sklearn import preprocessing
from sklearn.utils import shuffle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit
from mlxtend.evaluate import bias_variance_decomp
from sklearn.model_selection import cross_val_score #to Evaluate a score by cross-validation.
from sklearn.model_selection import KFold #Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).
from sklearn.metrics import confusion_matrix #to Compute confusion matrix to evaluate the accuracy of a classification.
from sklearn.metrics import accuracy_score #to compute Accuracy classification score.
from sklearn.metrics import classification_report #to Build a text report showing the main classification metrics.
from sklearn.metrics import plot_confusion_matrix #to polt the confusion matrix for visualization
from mlxtend.evaluate import bias_variance_decomp # for various loss functions.
from sklearn.decomposition import TruncatedSVD
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from nltk.tokenize import sent_tokenize # tokenize clean text into sentences not words
from sentence_transformers import SentenceTransformer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.summarizers.luhn import LuhnSummarizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from __future__ import absolute_import
from __future__ import division , print_function, unicode_literals
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

"""## **Upload the dataset and explore the data**"""

#read the dataset
df = pd.read_csv("/content/dataset.csv")

#print head of data
df.head(5)

#describe our data
df.describe()

#get some information about our data
df.info()

"""## **Step1: Data Preprocessing step**"""

df['labelID'] = df['label'].factorize()[0]
df

part = { # mapping treebank to wordnet lemmatizer 
    'N' : 'n',
    'V' : 'v',
    'J' : 'a',
    'S' : 's',
    'R' : 'r'
}

def get_tag(tag): #  used for lemmatizing 
  if tag[0] in part.keys():
    return part[tag[0]]
  else:
    return 'n'

#data pre-processing and data cleaning
stop_words = nltk.corpus.stopwords.words("english")
def data_preprocessing(corpus, stemming=False, lemmitization=True,printt = False):
    corpus = re.sub(r'[^,.’\'\w\s]', ' ', str(corpus).lower().strip()) #keep commas and fullstops for sentence 
    #corpus = re.sub(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", " ", corpus) # this line removes commas i don't know why
    corpus = re.sub("\d+", " ", corpus)           
    allText = corpus.split()
    if printt:
        print(f'number of words before removing stop words = {len(allText)}')
    if stop_words is not None:
        allText = [word for word in allText if word not in stop_words]
    if printt:   
        print(f'number of words after removing stop words = {len(allText)}')                
    if stemming == True:
        ps = nltk.stem.porter.PorterStemmer()
        allText = [ps.stem(word) for word in allText]
    if lemmitization == True:
        lem = nltk.stem.wordnet.WordNetLemmatizer()
        tagged = nltk.pos_tag(allText)
        allText = [lem.lemmatize(w[0], get_tag(w[1])) for w in tagged] # give lemmatizer part of speech
            
    corpus = " ".join(allText)
    return corpus

from sklearn.feature_extraction.text import CountVectorizer
df["cleaned_text"] = df["text"].apply(lambda txt: data_preprocessing(txt,stemming=False, lemmitization=True))
df['tokenized'] = df['cleaned_text'].apply(lambda x: nltk.tokenize.word_tokenize(x))

df["cleaned_Text_summary"] = df["Text_summary"].apply(lambda txt: data_preprocessing(txt,stemming=False, lemmitization=True))

df.head()

df['summary_len'] = [len(x) for x in df['Text_summary']]
df['text_len'] = [len(x) for x in df['text']]
df.head()

cleaned_df = df[df['text_len'] > df['summary_len']] # remove rows where text is shorter than summmary
print(len(cleaned_df))

cleaned_df['sentences'] = [x.split(',') for x in cleaned_df['cleaned_text']] # split text into sentences to give each one a score
cleaned_df.head()

cleaned_df.loc[0,'sentences']

cleaned_df.loc[0, 'tokenized']

"""## **Show the wordCloud of the data**"""

import wordcloud #Python wordcloud library to create tag clouds
import string
#for loop to take every unique book in the column of label 
for n in cleaned_df['label'].unique():
  word_cloud = cleaned_df[cleaned_df["label"]==n]["cleaned_text"]
 #to print the most frequent 50 words of the unique label
  print(f"\n THE MOST FREQUENT 50 WORDS OF LABEL CALLED: {n}\n")
  WordCloudGragh = wordcloud.WordCloud(background_color='black', max_words=50, max_font_size=40)
  WordCloudGragh = WordCloudGragh.generate(str(word_cloud))
  plt.axis('off')
  plt.imshow(WordCloudGragh, cmap=None)
  plt.show()

"""## **Split the dataset into train and test split**"""

#split dataset into subsets that minimize the potential for bias in your evaluation and validation process.
from sklearn.model_selection import train_test_split
x= np.array(cleaned_df['cleaned_text'])
y=np.array(cleaned_df['labelID'])
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state= 0)

"""## **Step 2: Perform Text Feature Engineering** """

from sklearn.feature_extraction.text import CountVectorizer #to Convert a collection of text documents to a matrix of token counts.
countVector= CountVectorizer()

"""### **1- Bag of words text feature engineering**

**A bag of words is a representation of text that describes the occurrence of words within a document.**
"""

vectorizer = CountVectorizer(min_df=2)
BOWtraining= vectorizer.fit_transform(x_train)
BOWtesting = vectorizer.transform(x_test)

"""### **2- TF-IDF text feature engineering**

Term frequency (TF) vectors show how important words are to documents. They are computed by using: ![image.png](data:image/png;base64,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)
"""

from sklearn.feature_extraction.text import TfidfVectorizer
TF_IDF = TfidfVectorizer(min_df=6,norm='l2',smooth_idf=True,use_idf=True)
TFIDF_Train = TF_IDF.fit_transform(x_train)
TFIDF_Test = TF_IDF.transform(x_test)

"""## **Step 3:Train the 3 Classification models**

### **1- perform Bag Of Words (BOW) transformation and apply it to different 3 classification models**:

1.   Support vector machine(SVM) classification Model
2.   Decision Tree Classification Model
3.   KNN classification Model

#### **1- Train SVM model with BOW**
"""

#define the type of support vector machine algorithm
BagOfWords_SVM = svm.SVC(kernel='linear')
BagOfWords_SVM.fit(BOWtraining,y_train)

SVM_BOW_Prediction = BagOfWords_SVM.predict(BOWtesting)

from sklearn.metrics import classification_report #to use the function of classification report
print(classification_report(y_train,BagOfWords_SVM.predict(BOWtraining)))

#testing classification report
print(classification_report(y_test,SVM_BOW_Prediction))

plot_confusion_matrix(BagOfWords_SVM ,BOWtesting , y_test)

"""#### **2- Train Decision Tree Classification model with BOW**"""

from sklearn.tree import DecisionTreeClassifier #for DT model
BOW_DT = DecisionTreeClassifier(random_state=0)
BOW_DT.fit(BOWtraining,y_train)
BOW_DT_Prediction = BOW_DT.predict(BOWtesting)

print(classification_report(y_train,BOW_DT.predict(BOWtraining)))

print(classification_report(y_test,BOW_DT_Prediction))

plot_confusion_matrix(BOW_DT ,BOWtesting , y_test)

"""#### **3- Train KNN classification with BOW**"""

from sklearn.neighbors import KNeighborsClassifier
BagOfWords_KNN = KNeighborsClassifier(n_neighbors = 3, algorithm= 'kd_tree', p= 1)
BagOfWords_KNN.fit(BOWtraining,y_train)
BOW_KNN_Prediction = BagOfWords_KNN.predict(BOWtesting)

print(classification_report(y_train,BagOfWords_KNN.predict(BOWtraining)))

print(classification_report(y_test,BOW_KNN_Prediction))

plot_confusion_matrix(BagOfWords_KNN ,BOWtesting , y_test)

"""### **2- perform TF-IDF transformation and apply it to different 3 classification models**:

1.   Support vector machine(SVM) classification Model
2.   Decision Tree Classification Model
3.   KNN classification Model

#### **1- Train SVM model with TF-IDF**
"""

#define the type of support vector machine algorithm
TFIDF_SVM = svm.SVC(kernel='linear')
TFIDF_SVM.fit(TFIDF_Train,y_train)

SVM_TFIDF_Prediction = TFIDF_SVM.predict(TFIDF_Test)

print(classification_report(y_train,TFIDF_SVM.predict(TFIDF_Train)))

#testing classification report
print(classification_report(y_test,SVM_TFIDF_Prediction))

plot_confusion_matrix(TFIDF_SVM ,TFIDF_Test , y_test)

"""#### **2- Train Decision Tree Classification model with TFIDF**"""

TFIDF_DT = DecisionTreeClassifier(random_state=0)
TFIDF_DT.fit(TFIDF_Train,y_train)
TFIDF_DT_Prediction = TFIDF_DT.predict(TFIDF_Test)

print(classification_report(y_train,TFIDF_DT.predict(TFIDF_Train)))

print(classification_report(y_test,TFIDF_DT_Prediction))

plot_confusion_matrix(TFIDF_DT ,TFIDF_Test , y_test)

"""#### **3- Train KNN classification with TFIDF**"""

TFIDF_KNN = KNeighborsClassifier(n_neighbors = 3, algorithm= 'kd_tree', p= 1)
TFIDF_KNN.fit(TFIDF_Train,y_train)
TFIDF_KNN_Prediction = TFIDF_KNN.predict(TFIDF_Test)

print(classification_report(y_train,TFIDF_KNN.predict(TFIDF_Train)))

print(classification_report(y_test,TFIDF_KNN_Prediction))

plot_confusion_matrix(TFIDF_KNN ,TFIDF_Test , y_test)

"""## **Step4: Visualization of Classification models**"""

#calculate the accuracies of each model
BOW_SVM_accuracy = BagOfWords_SVM.score(BOWtesting, y_test)
BOW_DT_accuracy = BOW_DT.score(BOWtesting, y_test)
BOW_KNN_accuracy = BagOfWords_KNN.score(BOWtesting, y_test)

TFIDF_SVM_accuracy = TFIDF_SVM.score(TFIDF_Test, y_test)
TFIDF_DT_accuracy = TFIDF_DT.score(TFIDF_Test, y_test)
TFIDF_KNN_accuracy = TFIDF_KNN.score(TFIDF_Test, y_test)

Accuracies = [BOW_SVM_accuracy , TFIDF_SVM_accuracy,
              BOW_DT_accuracy, TFIDF_DT_accuracy,
              BOW_KNN_accuracy, TFIDF_KNN_accuracy,]
Accuracies_Labels= ['BOW_SVM', 'TFIDF_SVM',
                    'BOW_DT', 'TFIDF_DT',
                    'BOW_KNN', 'TFIDF_KNN',]
Final_Accuracies_List = pd.DataFrame([Accuracies_Labels ,Accuracies ])
Final_Accuracies_List

fig = plt.figure(figsize=(15, 8))
ax = fig.add_subplot(111)
for i in range(len(Accuracies_Labels)):
    plt.bar(Accuracies_Labels[i] ,Accuracies[i]*100)
plt.xlabel('Classification Model')
plt.ylabel('Accuracy')
plt.title('Performance Evaluation for Classification Models')
plt.show()

"""## **Step 5: Perform Error Analysis**"""

y_test

"""### **Error Analysis for models with BOW**"""

SVM_BOW_Prediction

BOW_DT_Prediction

BOW_KNN_Prediction

cleaned_df['cleaned_text'][35178]

cleaned_df['labelID'][35178]

cleaned_df['labelID'][1]

cleaned_df['cleaned_text'][1]

SVM_BOW_Prediction_wordCloud = wordcloud.WordCloud(background_color='black', max_words=100, max_font_size=35)
SVM_BOW_Prediction_wordCloud = SVM_BOW_Prediction_wordCloud.generate(str(cleaned_df['cleaned_text'][35178]))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(SVM_BOW_Prediction_wordCloud, cmap=None)
plt.show()

BOW_DT_Prediction_wordCloud = wordcloud.WordCloud(background_color='black', max_words=100, max_font_size=35)
BOW_DT_Prediction_wordCloud = BOW_DT_Prediction_wordCloud.generate(str(cleaned_df['cleaned_text'][1]))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(BOW_DT_Prediction_wordCloud, cmap=None)
plt.show()

"""### **Error Analysis for models with TFIDF**"""

SVM_TFIDF_Prediction

TFIDF_DT_Prediction

TFIDF_KNN_Prediction

cleaned_df['labelID'][2]

cleaned_df['cleaned_text'][2]

TFIDF_KNN_Prediction_wordCloud = wordcloud.WordCloud(background_color='black', max_words=100, max_font_size=35)
TFIDF_KNN_Prediction_wordCloud = TFIDF_KNN_Prediction_wordCloud.generate(str(cleaned_df['cleaned_text'][2]))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(TFIDF_KNN_Prediction_wordCloud, cmap=None)
plt.show()

"""## **Step 6: Perform Clustering**"""

def BuildingKMeansModel(clusters, X_data):
  kMeansModel= KMeans(n_clusters= clusters, init='k-means++', random_state=0)
  Y_Prediction = kMeansModel.fit_predict(X_data)
  return kMeansModel, Y_Prediction

def TSNEData(DesiredOutput):
  Tsna = TSNE(n_components= 2, random_state= 42)
  DataOfTSNE = Tsna.fit_transform(DesiredOutput) 
  return DataOfTSNE

def ClusteringVisualization(ClusteringModel, cleaned_df, Y_Prediction, em=True):
  # get centroids of kmeans cluster
  if em:
    centroids = np.empty(shape=(ClusteringModel.n_components, cleaned_df.shape[1]))
  else:
    centroids = ClusteringModel.cluster_centers_

  # we want to transform the rows and the centroids
  # todense return matrix
  matrix_data = csr_matrix(cleaned_df)
  all_Data = concatenate((matrix_data.todense(), centroids))

  n_clusters = 4

  plt.scatter([all_Data[:-n_clusters, 0]], [all_Data[:-n_clusters, 1]], c=Y_Prediction, cmap=plt.cm.Paired, marker= 'x')
  plt.scatter([all_Data[-n_clusters:, 0]], [all_Data[-n_clusters:, 1]], marker= 'o')
  plt.show()

from collections import Counter
def CountingClusters(predictedClusters):
  clustersNumber = Counter(predictedClusters)
  plt.bar(clustersNumber.keys(), clustersNumber.values())

"""### **1- Kmans with BOW**"""

DataOfBOW = TSNEData(BOWtraining)
KMeansWithBOW, KMeansWithBOWPrediction = BuildingKMeansModel(4,DataOfBOW)

KMeansWithBOW.cluster_centers_.argsort()[:, ::-1]

CountingClusters(KMeansWithBOWPrediction)

"""### **2- Kmaeans with TFIDF**"""

DataOfTFIDF = TSNEData(TFIDF_Train)
KMeansWithTFIDF, KMeansWithTFIDFPrediction = BuildingKMeansModel(4,DataOfTFIDF)

KMeansWithTFIDF.cluster_centers_.argsort()[:, ::-1]

CountingClusters(KMeansWithTFIDFPrediction)

"""## **7- Perform Error Analysis and choose the best number of k**"""

from sklearn.metrics import silhouette_score ,silhouette_samples #to Compute the mean Silhouette Coefficient of all samples.
import matplotlib.cm as cm
RangeOfClusters = [ 3, 4, 5, 6, 7, 8] #define the range of clusters
SilhouetteScores = []
#for loop on the range of the clusters to draw the silhouette score and samples 
for NumOfClusters in RangeOfClusters:
    #To create a subplot of 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(13, 4)
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(DataOfTFIDF) + (NumOfClusters + 1) * 10])
    #define the clusters of K-means clustering model
    clusters = KMeans(n_clusters=NumOfClusters, random_state=10)
    #to define the cluster labels to predict
    cluster_labels = clusters.fit_predict(DataOfTFIDF)
    AverageOfSilhouette = silhouette_score(DataOfTFIDF, cluster_labels)
    SilhouetteScores.append(AverageOfSilhouette)
    #to print thr average number of silhouette score
    print("For NumOfClusters =", NumOfClusters, "The average of the silhouette score is :", AverageOfSilhouette ,"\n")
    #To compute the silhouette scores for each sample
    SilhouetteValuesSamples = silhouette_samples(DataOfTFIDF, cluster_labels)
    y_lower = 10
    for i in range(NumOfClusters):
        SilhouetteValues = \
            SilhouetteValuesSamples[cluster_labels == i]
        #sort the values of sillhouette 
        SilhouetteValues.sort()
        ClusterSize = SilhouetteValues.shape[0]
        #set the value of y_upper
        y_upper = y_lower + ClusterSize
        color = cm.nipy_spectral(float(i) / NumOfClusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, SilhouetteValues, facecolor=color, edgecolor=color, alpha=0.7)
        #define the labels of silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * ClusterSize, str(i))
        # Compute the new y_lower for next plot
        y_lower = y_upper + 10 

    ax1.set_title("The silhouette plot for the various clusters")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")
    ax1.axvline(x=AverageOfSilhouette, color="red", linestyle="--") #vertical line for the average silhouette score of all the values
    ax1.set_yticks([])  
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    #to show the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / NumOfClusters)
    ax2.scatter(DataOfTFIDF[:, 0], DataOfTFIDF[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')
    #to label the clusters based on the center value
    centers = clusters.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',c="white", alpha=1, s=200, edgecolor='k') 
    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')
    #to set the titles of x and y labels
    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")
    plt.suptitle(("Silhouette analysis for K-Means clustering  " "with NumOfClusters = %d" % NumOfClusters ,"with average silhouette score:", AverageOfSilhouette ), fontsize=14, fontweight='bold')
plt.show()

"""## **8- Perform LSA Text Summarization**

Latent Semantic Analysis is a robust algebraic-Statistical method which extracts hidden semantic structures of words and sentences. it extracts the features that cannot be directly mentioned. These features are essential to data, but are not original features of the dataset. It is an unsupervised approach along with the usage of Natural Language Processing (NLP).
"""

summarizer_lsa = LsaSummarizer()
summarizer_lsa.stop_words = get_stop_words("english")

#get data foreach topic
def get_data(n):
  topic=[]
  lab=np.array(cleaned_df['labelID'])
  data=np.array(cleaned_df['cleaned_text'])
  for i in range(len(cleaned_df)):
    if lab[i] == n:
      topic.append(data[i])


  topic_sum=[]

  data_sum=np.array(cleaned_df['cleaned_Text_summary'])
  for i in range(len(cleaned_df)):
    if lab[i] == n:
      topic_sum.append(data[i]) 

  return  topic , topic_sum

data= np.array(cleaned_df['cleaned_text'])
cleaned_Text_summary=np.array(cleaned_df['cleaned_Text_summary'])

#LSA function 
def lSa(topic):
  summarize_topic=[]
  for i in range(len(topic)):
    parser = PlaintextParser.from_string(topic[i]  ,Tokenizer("english"))
    for sentens in summarizer_lsa(parser.document, 1 ) :
      summarize_topic.append(sentens)
  return  summarize_topic

topic1 , topic1_sum = get_data(0)

summarize_data = lSa(data)

summarize_data_lsa= pd.DataFrame(summarize_data)
summarize_data_lsa

cleaned_Text_summary= pd.DataFrame(cleaned_Text_summary )
cleaned_Text_summary

frames = [summarize_data_lsa,cleaned_Text_summary ]
result = pd.concat(frames, axis=1 )
display(result)

summarize_topic1 = lSa(topic1)
summarize_topic1= pd.DataFrame(summarize_topic1)
summarize_topic1

cleaned_Text_summary= pd.DataFrame(cleaned_Text_summary)
cleaned_Text_summary

!pip install rouge
from rouge import Rouge
ROUGE = Rouge()

summarize_data_lsa

flist=[]
rlist=[]
plist=[]
res=[]
for i in range(len(summarize_data_lsa)):
  str1=summarize_data_lsa.iloc[i].to_string()
  str2=cleaned_Text_summary.iloc[i].to_string()
  res.append(ROUGE.get_scores(str1, str2))
  # flist.append(res[i]['rouge-l']['f'])
  # rlist.append(res[i]['rouge-l']['r'])
  # plist.append(res[i]['rouge-l']['p'])

res

dfs = [res[i][0] for i in range(len(res))]

dfscore = pd.DataFrame.from_records(dfs)

dfscore

flist=[]
flist1=[]
flist2=[]
rlist=[]
plist=[]
res=[]
for i in range(len(summarize_data_lsa)):
  str1=summarize_data_lsa.iloc[i].to_string()
  str2=cleaned_Text_summary.iloc[i].to_string()
  res.append(ROUGE.get_scores(str1, str2))
  # flist.append(res[i]['rouge-l']['f'])
  # rlist.append(res[i]['rouge-l']['r'])
  # plist.append(res[i]['rouge-l']['p'])

dfscore
for k, row in dfscore.iterrows():
  flist.append(row["rouge-l"]['f'])
  flist1.append(row["rouge-1"]['f'])
  flist2.append(row["rouge-2"]['f'])
  rlist.append(row["rouge-l"]['r'])
  plist.append(row["rouge-l"]['p'])

flist= pd.DataFrame(flist)
flist1= pd.DataFrame(flist1)
flist2= pd.DataFrame(flist2)
rlist= pd.DataFrame(rlist)
plist= pd.DataFrame(plist)

count=0
for i in range(len(flist)):
 count+=flist[0].iloc[i]
   
mean=count /len(flist)
mean

count=0
for i in range(len(flist)):
 count+=flist1[0].iloc[i]
   
mean=count /len(flist)
mean

r=pd.DataFrame()
r['summarize_data_lsa']=summarize_data_lsa
r['cleaned_Text_summary']=cleaned_Text_summary
r['fscore_rouge_L']=flist
r['fscore_rouge_1']=flist1
r['fscore_rouge_2']=flist2

r

mean1=r['fscore_rouge_1'].mean()
mean1

mean2=r['fscore_rouge_2'].mean()
mean2

meanL=r['fscore_rouge_L'].mean()
meanL

ty =pd.DataFrame()
ty['fscore_rouge_1']=[mean1]
ty['fscore_rouge_2']=[mean2]
ty['fscore_rouge_L']=[meanL]
display(ty)

import spacy

spacy.load('en_core_web_sm')
nlp = spacy.load("en_core_web_lg")
simi =[]
for i in range(len(summarize_topic1)):
    s1 = nlp("result['summarize_topic1'][i]")
    s2 = nlp("result['cleaned_Text_summary'][i]")
    simi.append(s1.similarity(s2))

similarity= pd.DataFrame({'cleaned_Text_summary':simi} )
similarity

cleaned_df['cleaned_Text_summary'][4]

y = "Scanzoni says it will be several years before the state's basins are dismantled."

from rouge_score import rouge_scorer # measure Rouge1 and RougeL score
human_summary = cleaned_df_left['Text_summary'][22]
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
scores = scorer.score(text,human_summary)
print(scores)

"""## **9- Perform Bert Text summarization**"""

#function to tokenize text into sentences and get embeding for every sentence using sentence transformer
def text_to_sent_list(text, 
                      embedder = SentenceTransformer('distilbert-base-nli-mean-tokens'),
                      min_len=2):
  
    ''' Returns cleaned article sentences and BERT sentence embeddings'''
    
    #convert to list of sentences

    sents = sent_tokenize(text)
    #remove short sentences by threshhold                                                                                                
    sents_clean = [sentence for sentence in sents if len(sentence)> min_len]
    #remove entries with empty list
    sents_clean = [sentence for sentence in sents_clean if len(sentence)!=0]
    #embed sentences (deafult uses BERT SentenceTransformer)
    sents_embedding= np.array(embedder.encode(sents_clean, convert_to_tensor=True))
    
    return sents_clean, sents_embedding

sents_clean, sents_embedding = text_to_sent_list(cleaned_df['cleaned_text'][1]) #trying on second row of dataset
print(sents_clean) 
print(len(sents_clean)) #tokenized into 13 sentences
print(sents_embedding.shape) # each sentence is a 768 feature vector

from sklearn.cluster import KMeans
model = KMeans(n_clusters=5)
pred = model.fit_predict(X = sents_embedding)
print(pred) # cluster for k = 5 and  predict which sentence belongs to which cluster
print(len(model.cluster_centers_))

X = set(pred)
print(X)

#get eculidean distance between every sentence and each centroid
# get the index of the sentence closest to the centroid of every cluster
min_list=[]
from sklearn.metrics.pairwise import euclidean_distances 
for j in range(5): # match this number with k 
  min = euclidean_distances(model.cluster_centers_[j].reshape(1, -1), sents_embedding[0].reshape(1, -1))
  idx = 0 
  for i in range(len(pred)):
    dist =  euclidean_distances(model.cluster_centers_[j].reshape(1, -1), sents_embedding[i].reshape(1, -1))
    if dist<min:
      idx = i
  min_list.append(idx)
print(min_list)

import sentence_transformers # get the indices of these same sentences from the original text and concatenate forming the summary
org_text = sent_tokenize(cleaned_df['text'][1])
print(len(org_text))
index_summ = list(set(min_list))
text =""
for i in range(len(index_summ)):
  text += org_text[index_summ[i]]
print(text)

human_summary = cleaned_df['Text_summary'][1]

from rouge_score import rouge_scorer # measure Rouge1 and RougeL score
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
scores = scorer.score(text,human_summary)
print(scores)

cleaned_df['len_clean'] = [len(sent_tokenize(x)) for x in cleaned_df['cleaned_text']]
cleaned_df['len_org'] = [len(sent_tokenize(x)) for x in cleaned_df['text']]

cleaned_df_right = cleaned_df[cleaned_df['len_clean'] > cleaned_df['len_org']]
cleaned_df_left =  cleaned_df[cleaned_df['len_clean'] < cleaned_df['len_org']]
cleaned_df_special = cleaned_df[cleaned_df['len_clean'] == cleaned_df['len_org']]

def bert_with_kmeans(row, cleaned_df, k):
  sents_clean, sents_embedding = text_to_sent_list(cleaned_df['cleaned_text'][row]) #trying on second row of dataset
  print(sents_clean) 
  print(len(sents_clean)) #tokenized into 13 sentences
  if len(sents_clean) < k:
        k = 1
  print(sents_embedding.shape) # each sentence is a 768 feature vector
  from sklearn.cluster import KMeans
  model = KMeans(n_clusters=k)
  pred = model.fit_predict(X = sents_embedding)
  print(pred) # cluster for k = 5 and  predict which sentence belongs to which cluster
  print(len(model.cluster_centers_))
  X = set(pred)
  print(X)
  #get eculidean distance between every sentence and each centroid
  #get the index of the sentence closest to the centroid of every cluster
  min_list=[]
  from sklearn.metrics.pairwise import euclidean_distances 
  for j in range(k): # match this number with k 
    min = euclidean_distances(model.cluster_centers_[j].reshape(1, -1), sents_embedding[0].reshape(1, -1))
    idx = 0 
    for i in range(len(pred)):
      dist =  euclidean_distances(model.cluster_centers_[j].reshape(1, -1), sents_embedding[i].reshape(1, -1))
      if dist<min:
        idx = i
    min_list.append(idx)
  print(min_list)
  import sentence_transformers # get the indices of these same sentences from the original text and concatenate forming the summary
  org_text = sent_tokenize(cleaned_df['text'][row])
  print(len(org_text))
  index_summ = list(set(min_list))
  text =""
  if len(org_text) == len(sents_clean):
    for i in range(len(index_summ)):
      text += org_text[index_summ[i]]

  elif len(sents_clean) > len(org_text):
    for i in range(len(index_summ)):
      if index_summ[i] < len(org_text):
        text += org_text[index_summ[i]]
      else:
        adj_index = index_summ[i] - (len(sents_clean) - len(org_text))
        text += org_text[adj_index]
  else:
    for i in range(len(index_summ)):
      text += org_text[index_summ[i]]
    
  print(text)
  return text

cleaned_df_special.head()

cleaned_df_right.head()

cleaned_df_left.head()

cleaned_df[cleaned_df['len_clean']< 5]

text = bert_with_kmeans(22, cleaned_df_left, 5)

from rouge_score import rouge_scorer # measure Rouge1 and RougeL score
human_summary = cleaned_df_left['Text_summary'][22]
scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
scores = scorer.score(text,human_summary)
print(scores)

"""## **10- Error Analysis for BERT text summarization**"""

cleaned_df_test = cleaned_df.groupby('labelID').sample(500, random_state = 0)
cleaned_df_test.labelID.value_counts()

Summaries = []
Scores = []
c = 1
for i in cleaned_df_test.index:
    print(c)
    text = bert_with_kmeans(i, cleaned_df_test, 5)
    human_summary = cleaned_df_test['Text_summary'][i]

    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(text,human_summary)

    Summaries.append(text)
    Scores.append(scores)
    c += 1

cleaned_df_test['Summaries'] = Summaries
cleaned_df_test['Scores'] = Scores
cleaned_df_test.head()

cleaned_df_test['Scores'][5520]['rougeL'][2]

f1_scores_rouge1 = []
f1_scores_rougeL = []
for i in cleaned_df_test.index:
    f1_scores_rouge1.append(cleaned_df_test['Scores'][i]['rouge1'][2])
    f1_scores_rougeL.append(cleaned_df_test['Scores'][i]['rougeL'][2])
cleaned_df_test['f1_rouge1'] = f1_scores_rouge1 
cleaned_df_test['f1_rougeL'] = f1_scores_rougeL

df_suboptimal = cleaned_df_test[(cleaned_df_test['f1_rouge1'] < 0.1) & (cleaned_df_test['f1_rougeL'] < 0.1) ]
len(df_suboptimal)

df_suboptimal['labelID'].value_counts()

df_sub_0 = df_suboptimal[df_suboptimal['labelID'] == 0]
len(df_sub_0)

summary_words = ""
for i in df_sub_0.index:
    summary_words += df_sub_0['Text_summary'][i]
    summary_words += " "

!pip install wordcloud

print(wordcloud.words_.keys())

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

df_sub_1 = df_suboptimal[df_suboptimal['labelID'] == 1]
len(df_sub_1)

summary_words = ""
for i in df_sub_1.index:
    summary_words += df_sub_1['Text_summary'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

df_sub_2 = df_suboptimal[df_suboptimal['labelID'] == 2]
len(df_sub_2)

summary_words = ""
for i in df_sub_2.index:
    summary_words += df_sub_2['Text_summary'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

df_sub_3 = df_suboptimal[df_suboptimal['labelID'] == 3]
len(df_sub_3)

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""**According to human summary word clouds these rows to have relevant information each according to its own class**

**lets see what the machine managed to capture**
"""

summary_words = ""
for i in df_sub_0.index:
    summary_words += df_sub_0['Summaries'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""**machine is focused more on names of news outlets and names of stock markets more than words correlated with the bussiness world**"""

summary_words = ""
for i in df_sub_1.index:
    summary_words += df_sub_1['Summaries'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""**The human summary focused on what is going to happen and when while the the machine was distracted to flashy news titles like Amazing, Go, News.**"""

summary_words = ""
for i in df_sub_2.index:
    summary_words += df_sub_2['Summaries'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""**The human summary focused on food safety and the diseases related to corruption of food crops while the machine was distracted by where the news was found like twitter, podcast or wesite, but still managed to capture scary and rights which are in a way related to the topics in this rows but not enough.**"""

summary_words = ""
for i in df_sub_3.index:
    summary_words += df_sub_3['Summaries'][i]
    summary_words += " "

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
stopwords = set(STOPWORDS)

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                max_words = 50,
                min_font_size = 10).generate(summary_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""**machine seemed to have focused on unnecessary details here,that may be included in an article but focusing on the importance of technology instead of the real news like google did so or Apple released something new**

## **11- Question and Answering System**
"""

tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

import torch
def answer_question(question, answer_text):
    input_ids = tokenizer.encode(question, answer_text)
    print('Query has {:,} tokens.\n'.format(len(input_ids)))
    sep_index = input_ids.index(tokenizer.sep_token_id)
    num_seg_a = sep_index + 1
    num_seg_b = len(input_ids) - num_seg_a
    segment_ids = [0]*num_seg_a + [1]*num_seg_b
    assert len(segment_ids) == len(input_ids)
    outputs = model(torch.tensor([input_ids]), 
                    token_type_ids=torch.tensor([segment_ids]), 
                    return_dict=True) 

    start_scores = outputs.start_logits
    end_scores = outputs.end_logits
    answer_start = torch.argmax(start_scores)
    answer_end = torch.argmax(end_scores)
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer = tokens[answer_start]
    for i in range(answer_start + 1, answer_end + 1):        
        if tokens[i][0:2] == '##':
            answer += tokens[i][2:]        
        else:
            answer += ' ' + tokens[i]
    print('Answer: "' + answer + '"')
    return answer

question = " what does Scanzoni says?"
answer=answer_question(question, y)

"""## **12- Machine Translation -------> Innovation**

1. English to German language Translation.
2. English to Chineese language Translation.
3. English to Arabic language Translation.

### **1. English to German language Translation.**
"""

src = "en"
dst = "de"
task_name = f"translation_{src}_to_{dst}"
model_name = f"Helsinki-NLP/opus-mt-{src}-{dst}"
translator  = pipeline(task_name, model=model_name, tokenizer=model_name)

translator("The energy giant says it is committed to cleaning up the Dan River spill site and will not increase customer rates to cover the expense.\nDuke Energy is adding a new element to dealing with its coal ash basins.\nWednesday, the company announced it will hire an independent team of engineers to review the condition of its sites.\nSpokesman Dave Scanzoni says the team will begin working as soon as North Carolina’s Governor Pat McCrory approves the company’s proposal.\n If we see any issues that need to be addressed, we’ll take care of them immediately.”This effort is in response the Dan River spill on February 2.")[0]["translation_text"]

text_to_translate = "The energy giant says it is committed to cleaning up the Dan River spill site and will not increase customer rates to cover the expense.\nDuke Energy is adding a new element to dealing with its coal ash basins.\nWednesday, the company announced it will hire an independent team of engineers to review the condition of its sites.\nSpokesman Dave Scanzoni says the team will begin working as soon as North Carolina’s Governor Pat McCrory approves the company’s proposal.\n If we see any issues that need to be addressed, we’ll take care of them immediately.”This effort is in response the Dan River spill on February 2."

def get_translation_model_and_tokenizer(src_lang, dst_lang):
  model_name = f"Helsinki-NLP/opus-mt-{src}-{dst}"
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
  return model, tokenizer

"""### **2. English to Chineese language Translation.**"""

src = "en"
dst = "zh"
model, tokenizer = get_translation_model_and_tokenizer(src, dst)

inputs = tokenizer.encode(text_to_translate, return_tensors="pt", max_length=512, truncation=True)
print(inputs)

# generate the translation output using greedy search
greedy_outputs = model.generate(inputs)
print(tokenizer.decode(greedy_outputs[0], skip_special_tokens=True))

# generate the translation output using beam search
beam_outputs = model.generate(inputs, num_beams=3)
print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))

"""### **3. English to Arabic language Translation.**"""

src = "en"
dst = "ar"
model, tokenizer = get_translation_model_and_tokenizer(src, dst)

# tokenize the text
inputs = tokenizer.encode(text_to_translate, return_tensors="pt", max_length=512, truncation=True)
beam_outputs = model.generate(
    inputs, 
    num_beams=5, 
    num_return_sequences=1,
    early_stopping=True,
)
for i, beam_output in enumerate(beam_outputs):
  print(tokenizer.decode(beam_output, skip_special_tokens=True))